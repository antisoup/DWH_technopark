{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "top_year = \"1992\"\n",
    "output_path = \"/user/n.mokrenko/hometask_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.window as w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем датасеты из таблиц базы данных tpch_flat_orc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationDF = spark.read.table(\"tpch_flat_orc_2.nation\")\n",
    "customerDF = spark.read.table(\"tpch_flat_orc_2.customer\")\n",
    "regionDF = spark.read.table(\"tpch_flat_orc_2.region\")\n",
    "ordersDF = spark.read.table(\"tpch_flat_orc_2.orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем новый датафрейм с топ-n пользователей с максимальным количеством заказов за год top_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_cond = lambda cond: sf.sum(sf.when(cond, 1).otherwise(0)) # функция проверки соответсвия значения ячейки значению cond\n",
    "joined_df = ordersDF.join(\n",
    "    customerDF, ordersDF.o_custkey == customerDF.c_custkey, 'left').withColumn( \n",
    "    'top_year',sf.lit(top_year)).groupBy('c_custkey').agg( # добавляем колонку top_year, группируем по Id-шнику\n",
    "    cnt_cond((sf.col('o_custkey') == sf.col('c_custkey')) & ordersDF.o_orderdate.like('1992%')).alias('orders_count'), #столбец orders_count, содержащий количество заказов пользователя\n",
    "    sf.min(sf.col('top_year')).alias('top_year')\n",
    ").orderBy(\n",
    "    sf.desc('orders_count')).limit(top_n).withColumn( # сортируем по убыванию количества заказов\n",
    "    'row_number', sf.row_number().over(w.Window.orderBy('top_year')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Джойним таблицу покупателей с таблицей стран, а затем с таблицей регионов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_join = customerDF.join(\n",
    "    nationDF, customerDF.c_nationkey == nationDF.n_nationkey).join(\n",
    "    regionDF, nationDF.n_regionkey == regionDF.r_regionkey).select( # деламем select нужных нам столбцов    \n",
    "    sf.col('c_custkey'),\n",
    "    sf.col('c_name').alias('customer_name'),\n",
    "    sf.col('n_name').alias('customer_nation_name'),\n",
    "    sf.col('r_name').alias('customer_region_name')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соединяем два предыдущих датафрейма, получаем таблицу с искомыми значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = joined_df.join(\n",
    "    cust_join, cust_join.c_custkey == joined_df.c_custkey).select(\n",
    "        'top_year','row_number', 'customer_name', 'customer_nation_name', 'customer_region_name','orders_count'\n",
    "    ).orderBy('row_number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем csv с нашим окончательным датафреймом и записываем его в output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.sortWithinPartitions('row_number').coalesce(1).write.csv(path=output_path, mode=\"overwrite\", sep=\"\\t\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+--------------------+--------------------+------------+\n",
      "|top_year|row_number|     customer_name|customer_nation_name|customer_region_name|orders_count|\n",
      "+--------+----------+------------------+--------------------+--------------------+------------+\n",
      "|    1992|         1|Customer#000093871|           INDONESIA|                ASIA|          13|\n",
      "|    1992|         2|Customer#000283198|              RUSSIA|              EUROPE|          13|\n",
      "|    1992|         3|Customer#000214702|               CHINA|                ASIA|          12|\n",
      "|    1992|         4|Customer#000265180|                IRAN|         MIDDLE EAST|          12|\n",
      "|    1992|         5|Customer#000293968|       UNITED STATES|             AMERICA|          12|\n",
      "+--------+----------+------------------+--------------------+--------------------+------------+"
     ]
    }
   ],
   "source": [
    "spark.read.csv(output_path, header=True, sep = '\\t').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
